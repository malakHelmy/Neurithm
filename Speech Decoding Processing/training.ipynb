{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TE6MdIeInqsh",
        "outputId": "3addb6cb-7381-4129-8f26-15af5ac783bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['#refs#', 'X_data', 'y_data']\n",
            "<HDF5 dataset \"y_data\": shape (1035, 1), type \"|O\">\n"
          ]
        }
      ],
      "source": [
        "import h5py\n",
        "file_path = '/content/drive/MyDrive/processed_data.mat'\n",
        "with h5py.File(file_path, 'r') as mat_data:\n",
        "    print(list(mat_data.keys()))  # List available variables\n",
        "    print(mat_data['y_data'])  # Print dataset metadata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKt6-OaJpR8C",
        "outputId": "2c5404e3-73c3-4386-ab6e-c39769ec5dfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_data shape: (1035, 256, 61)\n",
            "y_data length: 1035\n",
            "Example phoneme labels: ['pa' 'pe' 'bi' 'bi' 'bi' 'bi' 'ta' 'de' 'di' 'pi']\n"
          ]
        }
      ],
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "# Load the .mat file\n",
        "file_path = '/content/drive/MyDrive/processed_data.mat'  # Adjust path if needed\n",
        "with h5py.File(file_path, 'r') as mat_data:\n",
        "    X_data = np.array(mat_data['X_data'])  # Load EEG data\n",
        "\n",
        "    # Extract and decode phoneme labels correctly\n",
        "    y_data = []\n",
        "    y_data_ref = mat_data['y_data']  # This is an array of references\n",
        "\n",
        "    for i in range(y_data_ref.shape[0]):  # Loop over stored labels\n",
        "        ref = y_data_ref[i][0]  # Get reference to actual dataset\n",
        "        label_data = mat_data[ref][:]  # Dereference and load data\n",
        "        label = ''.join(map(chr, label_data.flatten()))  # Decode bytes to string\n",
        "        y_data.append(label)\n",
        "\n",
        "# Convert to NumPy array\n",
        "y_data = np.array(y_data)\n",
        "\n",
        "# Check results\n",
        "print(\"X_data shape:\", X_data.shape)\n",
        "print(\"y_data length:\", len(y_data))\n",
        "print(\"Example phoneme labels:\", y_data[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import (Input, Conv2D, BatchNormalization, MaxPooling2D, Flatten, Dense,\n",
        "                                     Dropout, LSTM, TimeDistributed, Reshape, GlobalAveragePooling1D,\n",
        "                                     Bidirectional)\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import random\n",
        "\n",
        "# Load EEG Data\n",
        "file_path = '/content/drive/MyDrive/processed_data (1).mat'\n",
        "with h5py.File(file_path, 'r') as mat_data:\n",
        "    X_data = np.array(mat_data['X_data'])\n",
        "    y_data_ref = mat_data['y_data']\n",
        "\n",
        "    y_data = []\n",
        "    for i in range(y_data_ref.shape[0]):\n",
        "        ref = y_data_ref[i][0]\n",
        "        label_data = mat_data[ref][:]\n",
        "        label = ''.join(map(chr, label_data.flatten()))\n",
        "        y_data.append(label)\n",
        "\n",
        "# Sort EEG Data by Phoneme Label\n",
        "sorted_indices = np.argsort(y_data)\n",
        "X_data = X_data[sorted_indices]\n",
        "y_data = np.array(y_data)[sorted_indices]\n",
        "\n",
        "# Convert Labels to One-Hot Encoding\n",
        "unique_labels = sorted(list(set(y_data)))\n",
        "label_to_index = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "y_encoded = np.array([label_to_index[label] for label in y_data])\n",
        "y_onehot = to_categorical(y_encoded, num_classes=len(unique_labels))\n",
        "\n",
        "# Reshape EEG Data for CNN+LSTM\n",
        "X_data = np.expand_dims(X_data, axis=-1)\n",
        "\n",
        "# Data Augmentation Functions\n",
        "def add_noise(data, noise_level=0.02):\n",
        "    return data + np.random.normal(0, noise_level, data.shape)\n",
        "\n",
        "def time_shift(data, shift_max=10):\n",
        "    shift = random.randint(-shift_max, shift_max)\n",
        "    return np.roll(data, shift, axis=1)\n",
        "\n",
        "def scale_amplitude(data, scale_min=0.8, scale_max=1.2):\n",
        "    scale = np.random.uniform(scale_min, scale_max)\n",
        "    return data * scale\n",
        "\n",
        "def frequency_mask(data, mask_size=5):\n",
        "    data[:, :mask_size, :] = 0\n",
        "    return data\n",
        "\n",
        "# Applying Data Augmentation\n",
        "augmented_X = []\n",
        "augmented_y = []\n",
        "for i in range(len(X_data)):\n",
        "    augmented_X.append(X_data[i])\n",
        "    augmented_X.append(add_noise(X_data[i]))\n",
        "    augmented_X.append(time_shift(X_data[i]))\n",
        "    augmented_X.append(scale_amplitude(X_data[i]))\n",
        "    augmented_X.append(frequency_mask(X_data[i]))\n",
        "    for _ in range(5):\n",
        "        augmented_y.append(y_onehot[i])\n",
        "\n",
        "X_data = np.array(augmented_X)\n",
        "y_onehot = np.array(augmented_y)\n",
        "\n",
        "# Split Data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_data, y_onehot, test_size=0.2, random_state=42, stratify=y_onehot)\n",
        "\n",
        "# Compute Class Weights for Balanced Training\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_encoded), y=y_encoded)\n",
        "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
        "\n",
        "# CNN+BiLSTM Model\n",
        "def CNN_LSTM(input_shape, num_classes):\n",
        "    input_layer = Input(shape=input_shape)\n",
        "\n",
        "    # CNN Feature Extractor\n",
        "    x = Conv2D(32, (3,3), activation='relu', padding='same', kernel_regularizer=l2(0.001))(input_layer)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D((2,2))(x)\n",
        "\n",
        "    x = Conv2D(64, (3,3), activation='relu', padding='same', kernel_regularizer=l2(0.001))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D((2,2))(x)\n",
        "\n",
        "    x = Conv2D(128, (3,3), activation='relu', padding='same', kernel_regularizer=l2(0.001))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D((2,2))(x)\n",
        "\n",
        "    x = TimeDistributed(Flatten())(x)\n",
        "    x = Reshape((x.shape[1], x.shape[-1]))(x)\n",
        "\n",
        "    # Bidirectional LSTM\n",
        "    x = Bidirectional(LSTM(64, return_sequences=True, kernel_regularizer=l2(0.001)))(x)\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "\n",
        "    # Output Layer\n",
        "    output = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output)\n",
        "    return model\n",
        "\n",
        "# Building Model\n",
        "input_shape = (X_train.shape[1], X_train.shape[2], 1)\n",
        "num_classes = len(unique_labels)\n",
        "model = CNN_LSTM(input_shape, num_classes)\n",
        "\n",
        "# Compiling Model\n",
        "model.compile(optimizer=AdamW(learning_rate=0.001, weight_decay=1e-4),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-5)\n",
        "\n",
        "# Training Model\n",
        "history = model.fit(X_train, y_train,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    epochs=100,\n",
        "                    batch_size=512,\n",
        "                    class_weight=class_weight_dict,\n",
        "                    callbacks=[early_stopping, reduce_lr],\n",
        "                    verbose=1)\n",
        "\n",
        "# Saving Model\n",
        "model.save('/content/drive/MyDrive/CNN_BiLSTM_enhanced(1).h5')\n",
        "\n",
        "# Evaluate Model\n",
        "test_loss, test_acc = model.evaluate(X_val, y_val)\n",
        "print(f\"Test Accuracy: {test_acc:.2f}\")\n",
        "\n",
        "# Predicting and Printing First 5 Phonemes\n",
        "predictions = model.predict(X_val)\n",
        "predicted_indices = np.argmax(predictions, axis=1)\n",
        "true_indices = np.argmax(y_val, axis=1)\n",
        "\n",
        "for i in range(5):\n",
        "    print(f\"Predicted: {unique_labels[predicted_indices[i]]} | True: {unique_labels[true_indices[i]]}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
